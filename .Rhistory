hansard <- posextract.adj_noun_pairs$extract(hansard, col) # change to (hansard, ...) maybe
hansard[] <- lapply(hansard, as.vector)
return(hansard) }
extract_adj_noun_pairs()
#' @export
extract_adj_noun_pairs <- function(hansard, col) {
#posextract <- reticulate::import_from_path("posextract.extract_adj_noun_pairs", "~/projects/posextractr/") # I won't need this once posextract has been submitted to pypi
posextract <- import("posextract")
# parse here in R, then pass parsed object to extract_adj_noun_pairs()
hansard <- adj_noun_pairs$extract(hansard, col) # change to (hansard, ...) maybe
hansard[] <- lapply(hansard, as.vector)
return(hansard) }
extract_adj_noun_pairs()
#' @export
extract_adj_noun_pairs <- function(hansard, col) {
#posextract <- reticulate::import_from_path("posextract.extract_adj_noun_pairs", "~/projects/posextractr/") # I won't need this once posextract has been submitted to pypi
posextract <- import("posextract")
# parse here in R, then pass parsed object to extract_adj_noun_pairs()
hansard <- posextract$adj_noun_pairs$extract(hansard, col) # change to (hansard, ...) maybe
hansard[] <- lapply(hansard, as.vector)
return(hansard) }
extract_adj_noun_pairs()
posextract <- import("posextract")
py_list_attributes(posextract)
#' @export
extract_adj_noun_pairs <- function(hansard, col) {
#posextract <- reticulate::import_from_path("posextract.extract_adj_noun_pairs", "~/projects/posextractr/") # I won't need this once posextract has been submitted to pypi
posextract <- import("posextract")
# parse here in R, then pass parsed object to extract_adj_noun_pairs()
hansard <- posextract$adj_noun_pairs$extract(hansard, col) # change to (hansard, ...) maybe
hansard[] <- lapply(hansard, as.vector)
return(hansard) }
extract_adj_noun_pairs()
#' @export
extract_adj_noun_pairs <- function(hansard, col) {
#posextract <- reticulate::import_from_path("posextract.extract_adj_noun_pairs", "~/projects/posextractr/") # I won't need this once posextract has been submitted to pypi
posextract <- import("posextract")
# parse here in R, then pass parsed object to extract_adj_noun_pairs()
hansard <- posextract$adj_noun_pairs$extract(hansard, col) # change to (hansard, ...) maybe
hansard[] <- lapply(hansard, as.vector)
return(hansard) }
extract_adj_noun_pairs()
os <- import("os")
os <- import("pandas")
os <- import("posextract")
os <- import("pandas")
py_install("pandas")
py_install("posextract'")
py_install("posextract")
posextract <- import("posextract")
py_install("bloop")
py_install("bloophjghghj")
py_install("posextract")
library("reticulate")
#' @export
posextract_install <- function() {
if(dir.exists(paste0(virtualenv_root(), '/r-posextract'))) {
print("posextract has already been installed. Reinstall?")
print("Press 1 for reinstall")
print("Press 2 to pass")
input <- readline(prompt="Select Option: ")
if(input == 1) {
virtualenv_create("r-posextract", install_python(), packages = c("pandas", "spacy", "posextract"))
#system(command = "python -m spacy download en_core_web_sm")
}
else if(input == 2) {
print("Skipping Install") }
else {
print("Not a valid option. Exiting.") } }
else {
virtualenv_create("r-posextract", install_python(), packages = c("pandas", "spacy"))
system(command = "python -m spacy download en_core_web_sm") } }
#' @export
extract_adj_noun_pairs <- function(hansard, col) {
#posextract <- reticulate::import_from_path("posextract.extract_adj_noun_pairs", "~/projects/posextractr/") # I won't need this once posextract has been submitted to pypi
posextract <- import("posextract")
# parse here in R, then pass parsed object to extract_adj_noun_pairs()
hansard <- posextract$adj_noun_pairs$extract(hansard, col) # change to (hansard, ...) maybe
hansard[] <- lapply(hansard, as.vector)
return(hansard) }
#' @export
extract_adj_noun_pairs <- function(hansard, col) {
#posextract <- reticulate::import_from_path("posextract.extract_adj_noun_pairs", "~/projects/posextractr/") # I won't need this once posextract has been submitted to pypi
#posextract <- import("posextract")
# parse here in R, then pass parsed object to extract_adj_noun_pairs()
hansard <- posextract$adj_noun_pairs$extract(hansard, col) # change to (hansard, ...) maybe
hansard[] <- lapply(hansard, as.vector)
return(hansard) }
#' @export
posextract_install <- function() {
if(dir.exists(paste0(virtualenv_root(), '/r-posextract'))) {
print("posextract has already been installed. Reinstall?")
print("Press 1 for reinstall")
print("Press 2 to pass")
input <- readline(prompt="Select Option: ")
if(input == 1) {
virtualenv_create("r-posextract", install_python(), packages = c("pandas", "spacy", "posextract"))
#system(command = "python -m spacy download en_core_web_sm")
}
else if(input == 2) {
print("Skipping Install") }
else {
print("Not a valid option. Exiting.") } }
else {
virtualenv_create("r-posextract", install_python(), packages = c("pandas", "spacy"))
system(command = "python -m spacy download en_core_web_sm") } }
#' @export
extract_adj_noun_pairs <- function(hansard, col) {
#posextract <- reticulate::import_from_path("posextract.extract_adj_noun_pairs", "~/projects/posextractr/") # I won't need this once posextract has been submitted to pypi
#posextract <- import("posextract")
# parse here in R, then pass parsed object to extract_adj_noun_pairs()
hansard <- posextract$adj_noun_pairs$extract(hansard, col) # change to (hansard, ...) maybe
hansard[] <- lapply(hansard, as.vector)
return(hansard) }
#' @export
posextract_initialize <- function() {
use_virtualenv("r-posextract") }
library("reticulate")
posextract_install()
posextract_initialize()
extract_adj_noun_pairs()
#' @export
extract_adj_noun_pairs <- function(hansard, col) {
#posextract <- reticulate::import_from_path("posextract.extract_adj_noun_pairs", "~/projects/posextractr/") # I won't need this once posextract has been submitted to pypi
posextract <- import("posextract")
# parse here in R, then pass parsed object to extract_adj_noun_pairs()
hansard <- posextract$adj_noun_pairs$extract(hansard, col) # change to (hansard, ...) maybe
hansard[] <- lapply(hansard, as.vector)
return(hansard) }
extract_adj_noun_pairs()
hansard <- read_csv("/home/stephbuon/data/samples/full_hansard_sample.csv")
library(tidyverse)
hansard <- read_csv("/home/stephbuon/data/samples/full_hansard_sample.csv")
extract_adj_noun_pairs(hansard, 'text')
system(command = "python -m spacy download en_core_web_sm")
a <- extract_adj_noun_pairs(hansard, 'text')
View(a)
require(devtools)
install_github("stephbuon/posextractr")
library(posextractr)
library(tidyverse)
hansard <- read_csv("/home/stephbuon/data/samples/full_hansard_sample.csv")
extract_adj_noun_pairs()
library('reticulate')
extract_adj_noun_pairs(hansard, 'text')
a <- extract_adj_noun_pairs(hansard, 'text')
library(hansardr)
data("hansard_1800")
hansard_sample <- hansard_1800 %>%
sample_n(100)
a <- extract_adj_noun_pairs(hansard_sample, 'text')
View(a)
library("hansardr")
data("hansard_1800")
hansard_sample <- hansard_1800 %>%
sample_n(100)
adj_noun_pairs_in_hansard <- extract_adj_noun_pairs(hansard_sample, "text")
adj_noun_pairs_count <- adj_noun_pairs_in_hansard %>%
count(adj_noun_pair)
ggplot(data = adj_noun_pairs_in_hansard,
aes(x = n, y = adj_noun_pair)) +
geom_bar(stat = "identity", width = 0.5) +
coord_flip()
aes(x = n, y = adj_noun_pair) +
)
ggplot(data = adj_noun_pairs_count,
aes(x = n, y = adj_noun_pair)) +
geom_bar(stat = "identity", width = 0.5) +
coord_flip()
ggplot(data = adj_noun_pairs_count) +
geom_col(aes(x = reorder(adj_noun_pair, n),
y = n),
fill = "steel blue") +
coord_flip() +
labs(title = "Top Lemmatized Adjective-Noun Pairs with Woman",
#subtitle = "From 19th-century British Parliamentary Debate Text",
caption = "Searching the Hansard Parliamentary Debates from 1870-79",
x = "Triple",
y = "count")
adj_noun_pairs_count <- adj_noun_pairs_in_hansard %>%
count(adj_noun_pair) %>%
arrange(desc(n)) %>%
slice(10)
ggplot(data = adj_noun_pairs_count) +
geom_col(aes(x = reorder(adj_noun_pair, n),
y = n),
fill = "steel blue") +
coord_flip() +
labs(title = "Top Lemmatized Adjective-Noun Pairs with Woman",
#subtitle = "From 19th-century British Parliamentary Debate Text",
caption = "Searching the Hansard Parliamentary Debates from 1870-79",
x = "Triple",
y = "count")
adj_noun_pairs_count <- adj_noun_pairs_in_hansard %>%
count(adj_noun_pair) %>%
arrange(desc(n))
adj_noun_pairs_count <- adj_noun_pairs_in_hansard %>%
count(adj_noun_pair) %>%
arrange(desc(n)) %>%
slice(10)
adj_noun_pairs_count <- adj_noun_pairs_in_hansard %>%
count(adj_noun_pair) %>%
arrange(desc(n)) %>%
slice(1:10)
adj_noun_pairs_count <- adj_noun_pairs_in_hansard %>%
count(adj_noun_pair) %>%
arrange(desc(n)) %>%
slice(:10)
adj_noun_pairs_count <- adj_noun_pairs_in_hansard %>%
count(adj_noun_pair) %>%
arrange(desc(n)) %>%
slice(1:10)
ggplot(data = adj_noun_pairs_count) +
geom_col(aes(x = reorder(adj_noun_pair, n),
y = n),
fill = "steel blue") +
coord_flip() +
labs(title = "Top Lemmatized Adjective-Noun Pairs with Woman",
#subtitle = "From 19th-century British Parliamentary Debate Text",
caption = "Searching the Hansard Parliamentary Debates from 1870-79",
x = "Triple",
y = "count")
library("hansardr")
data("hansard_1800")
hansard_sample <- hansard_1800 %>%
sample_n(100)
adj_noun_pairs_in_hansard <- extract_adj_noun_pairs(hansard_sample, "text")
adj_noun_pairs_count <- adj_noun_pairs_in_hansard %>%
count(adj_noun_pair) %>%
arrange(desc(n)) %>%
slice(1:10)
ggplot(data = adj_noun_pairs_count) +
geom_col(aes(x = reorder(adj_noun_pair, n),
y = n),
fill = "steel blue") +
coord_flip() +
labs(title = "Examples of Adjective-Noun Pairs",
caption = "Searching the 1800s Hansard Parliamentary Debates",
x = "Adjective-Noun Pair",
y = "Count")
library("hansardr")
data("hansard_1800")
hansard_sample <- hansard_1800 %>%
sample_n(100)
adj_noun_pairs_in_hansard <- extract_adj_noun_pairs(hansard_sample, "text")
adj_noun_pairs_count <- adj_noun_pairs_in_hansard %>%
count(adj_noun_pair) %>%
arrange(desc(n)) %>%
slice(1:10)
ggplot(data = adj_noun_pairs_count) +
geom_col(aes(x = reorder(adj_noun_pair, n),
y = n),
fill = "steel blue") +
coord_flip() +
labs(title = "Examples of Adjective-Noun Pairs",
caption = "Searching the Hansard Parliamentary Debates",
x = "Adjective-Noun Pair",
y = "Count")
library("hansardr")
data("hansard_1800")
hansard_sample <- hansard_1800 %>%
sample_n(100)
adj_noun_pairs_in_hansard <- extract_adj_noun_pairs(hansard_sample, "text")
adj_noun_pairs_count <- adj_noun_pairs_in_hansard %>%
count(adj_noun_pair) %>%
arrange(desc(n)) %>%
slice(1:10)
ggplot(data = adj_noun_pairs_count) +
geom_col(aes(x = reorder(adj_noun_pair, n),
y = n),
fill = "steel blue") +
coord_flip() +
labs(title = "Examples of Adjective-Noun Pairs",
caption = "Searching the Hansard Parliamentary Debates",
x = "Adjective-Noun Pair",
y = "Count")
adj_noun_pairs_count <- adj_noun_pairs_in_hansard %>%
count(adj_noun_pair) %>%
arrange(desc(n)) %>%
slice(1:20)
ggplot(data = adj_noun_pairs_count) +
geom_col(aes(x = reorder(adj_noun_pair, n),
y = n),
fill = "steel blue") +
coord_flip() +
labs(title = "Examples of Adjective-Noun Pairs",
caption = "Searching the Hansard Parliamentary Debates",
x = "Adjective-Noun Pair",
y = "Count")
library("hansardr")
data("hansard_1800")
hansard_sample <- hansard_1800 %>%
sample_n(100)
adj_noun_pairs_in_hansard <- extract_adj_noun_pairs(hansard_sample, "text")
adj_noun_pairs_count <- adj_noun_pairs_in_hansard %>%
count(adj_noun_pair) %>%
arrange(desc(n)) %>%
slice(1:20)
ggplot(data = adj_noun_pairs_count) +
geom_col(aes(x = reorder(adj_noun_pair, n),
y = n),
fill = "steel blue") +
coord_flip() +
labs(title = "Examples of Adjective-Noun Pairs",
caption = "Searching the Hansard Parliamentary Debates",
x = "Adjective-Noun Pair",
y = "Count")
shiny::runApp('congress-shiny')
runApp('congress-shiny')
runApp('congress-shiny')
shiny::runApp('congress-shiny')
data_dir <- "~/projects/congress-shiny/app/app-data/small_congress/"
runApp('congress-shiny')
shiny::runApp('congress-shiny')
data_dir <- "~/projects/congress-shiny/app/app-data/small_congress/"
runApp('congress-shiny')
runApp('congress-shiny')
shiny::runApp('congress-shiny')
data_dir <- "~/projects/congress-shiny/app/app-data/small_congress/"
runApp('congress-shiny')
runApp('congress-shiny')
runApp('congress-shiny')
library(tidyverse)
code_dir <- "~/projects/congress-shiny/preprocess-code/collocates/collocates-functions/"
source(paste0(code_dir, "clean_collocates.R"))
source(paste0(code_dir, "count_collocates.R"))
source(paste0(code_dir, "score_collocates.R"))
keyword <- "all"
#keyword <- "concerns"
#keyword <- "property"
data_dir <- "~/home/stephbuon/projects/congress-shiny/app/app-data/collocates/"
collocates <- read_csv(paste0(data_dir, keyword, "_adj_noun_collocates.csv"))
library(tidyverse)
code_dir <- "~/projects/congress-shiny/preprocess-code/collocates/collocates-functions/"
source(paste0(code_dir, "clean_collocates.R"))
source(paste0(code_dir, "count_collocates.R"))
source(paste0(code_dir, "score_collocates.R"))
keyword <- "all"
#keyword <- "concerns"
#keyword <- "property"
data_dir <- "~/projects/congress-shiny/app/app-data/collocates/"
collocates <- read_csv(paste0(data_dir, keyword, "_adj_noun_collocates.csv"))
collocates <- clean_collocates(collocates, keyword)
collocates <- count_collocates(collocates)
if (keyword == "speakers") {
collocates <- searchable_speaker(collocates) }
collocates <- score_collocates(collocates, keyword)
write_csv(collocates, paste0(dir, "clean_", keyword, "_adj_noun_collocates.csv"))
write_csv(collocates, paste0(data_dir, "clean_", keyword, "_adj_noun_collocates.csv"))
shiny::runApp('congress-shiny')
data_dir <- "~/projects/congress-shiny/app/app-data/"
runApp('congress-shiny')
runApp('congress-shiny/app/modules/context/context.R')
runApp('congress-shiny/app/modules/context/context.R')
runApp('congress-shiny')
shiny::runApp('congress-shiny')
data_dir <- "~/projects/congress-shiny/app/app-data/"
runApp('congress-shiny')
shiny::runApp('hansard-shiny/app')
require(devtools)
install_github("stephbuon/hansardr")
require(devtools)
install_github("stephbuon/hansardr")
data(speaker_metadata_1800)
data("speaker_metadata_1800")
library(hansardr)
data("speaker_metadata_1800")
View(speaker_metadata_1800)
data("speaker_metadata_1870")
View(speaker_metadata_1870)
require(devtools)
install_github("stephbuon/hansardr")
library(hansardr)
data("hansard_1880")
load("/home/stephbuon/projects/hansardr/data/s/speaker_metadata_1800.RData")
data_dir <- "~/projects/congress-shiny/app/app-data/"
shiny::runApp('congress-shiny')
shiny::runApp('congress-shiny')
data_dir <- "~/projects/congress-shiny/app/app-data/"
runApp('congress-shiny')
knitr::opts_chunk$set(echo = TRUE)
library(hansardr)
library(tidyverse)
library(tidytext)
data("stop_words")
data("hansard_1830")
data("hansard_1860")
hansard_1830 <- hansard_1830 %>%
mutate(decade = 1830)
hansard_1860 <- hansard_1860 %>%
mutate(decade = 1860)
tokenized_hansard_data <- bind_rows(hansard_1830, hansard_1860)
tokenized_hansard_data <- tokenized_hansard_data %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "[:digit:]"))
hansard_count <- tokenized_hansard_data %>%
count(decade, word, sort = TRUE) %>%
filter(n > 1)
total_count <- hansard_count %>%
group_by(decade) %>%
summarize(total = sum(n))
hansard_words <- left_join(hansard_count, total_count)
hansard_tf_idf <- hansard_words %>%
bind_tf_idf(word, decade, n)
hansard_tf_idf <- hansard_tf_idf %>%
group_by(decade) %>%
slice_max(tf_idf, n = 15)
ggplot(hansard_tf_idf,
aes(tf_idf,
reorder(word, tf_idf),
fill = decade)) +
geom_col(show.legend = FALSE) +
facet_wrap(~decade, ncol = 2, scales = "free") +
labs(x = "TF-IDF", y = "Word")
data("speaker_metadata_1860")
hansard_1860 <- left_join(hansard_1860, debate_metadata_1860)
data("speaker_metadata_1860")
head("speaker_metadata_1860")
data("speaker_metadata_1860")
head(speaker_metadata_1860)
speaker_metadata_1860 <- speaker_metadata_1860 %>%
select(sentence_id, speaker, suggested_speaker)
hansard_1860 <- left_join(hansard_1860, speaker_metadata_1860)
speaker_count <- hansard_1860 %>%
filter(disambig_speaker %in% c("william_gladstone_3104", "james_whiteside_4119")) %>%
unnest_tokens(word, text)  %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "[:digit:]"))  %>%
count(disambig_speaker, word, sort = TRUE) %>%
filter(n > 1)
speaker_count <- hansard_1860 %>%
filter(suggested_speaker %in% c("william_gladstone_3104", "james_whiteside_4119")) %>%
unnest_tokens(word, text)  %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "[:digit:]"))  %>%
count(suggested_speaker, word, sort = TRUE) %>%
filter(n > 1)
total_count <- speaker_count %>%
group_by(disambig_speaker) %>%
summarize(total = sum(n))
speaker_count <- hansard_1860 %>%
filter(suggested_speaker %in% c("william_gladstone_3104", "james_whiteside_4119")) %>%
unnest_tokens(word, text)  %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "[:digit:]"))  %>%
count(suggested_speaker, word, sort = TRUE) %>%
filter(n > 1)
total_count <- speaker_count %>%
group_by(suggested_speaker) %>%
summarize(total = sum(n))
data("speaker_metadata_1860")
head(speaker_metadata_1860)
library(shiny); runApp('congress-shiny/app/modules/context/context.R')
runApp('congress-shiny')
data_dir <- "~/projects/congress-shiny/app/app-data/"
runApp('congress-shiny')
runApp('congress-shiny')
runApp('congress-shiny')
runApp('hansard-shiny/app')
runApp('hansard-shiny/app')
runApp('congress-shiny')
data_dir <- "~/projects/congress-shiny/app/app-data/"
runApp('congress-shiny')
library(tidyverse)
code_dir <- "~/projects/congress-shiny/preprocess-code/collocates/clean-collocates/collocates-functions/"
source(paste0(code_dir, "clean_collocates.R"))
source(paste0(code_dir, "count_collocates.R"))
source(paste0(code_dir, "score_collocates.R"))
keyword <- "all"
#keyword <- "concerns"
#keyword <- "property"
data_dir <- "~/projects/congress-shiny/app/app-data/context/"
collocates <- read_csv(paste0(data_dir, keyword, "_adj_noun_collocates.csv"))
library(tidyverse)
code_dir <- "~/projects/congress-shiny/preprocess-code/collocates/clean-collocates/collocates-functions/"
source(paste0(code_dir, "clean_collocates.R"))
source(paste0(code_dir, "count_collocates.R"))
source(paste0(code_dir, "score_collocates.R"))
keyword <- "all"
#keyword <- "concerns"
#keyword <- "property"
data_dir <- "~/projects/congress-shiny-data/app/app-data/context/"
collocates <- read_csv(paste0(data_dir, keyword, "_adj_noun_collocates.csv"))
library(tidyverse)
data_dir <- "~/projects/congress-shiny-data/app-data/context/"
collocates <- read_csv(paste0(data_dir, keyword, "_adj_noun_collocates.csv"))
collocates <- collocates %>% sample_n(100)
collocates$year <- format(collocates$year, format="%Y")
View(collocates)
collocates$year <- as.Date(format(collocates$year, format="%Y"))
collocates$year <- as.Numeric(format(collocates$year, format="%Y"))
collocates$year <- as.numeric(format(collocates$year, format="%Y"))
collocates <- collocates %>%
mutate(decade = year - year %% 10)
shiny::runApp('congress-shiny')
data_dir <- "~/projects/congress-shiny/app/app-data/"
runApp('congress-shiny')
runApp('congress-shiny')
runApp('congress-shiny')
shiny::runApp('congress-shiny')
data_dir <- "~/projects/congress-shiny/app/app-data/"
runApp('congress-shiny')
runApp('congress-shiny')
runApp('congress-shiny')
runApp('congress-shiny')
data_dir <- "~/projects/congress-shiny/app/app-data/"
runApp('congress-shiny')
runApp('congress-shiny')
runApp('congress-shiny')
runApp('congress-shiny')
runApp('congress-shiny')
